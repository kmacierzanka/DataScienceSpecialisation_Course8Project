---
title: "Practical Machine Learning Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

In this report a machine learning model (random forest) is built to predict the class `classe` to which a single barbell lift belongs, based on data collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. There are 5 classes (A, B, C, D and E), with class "A" being a correct barbell lift. This is a classification, and not a regression, problem.

### R version and required packages

```{r version}
Sys.Date()
version
```

```{r packages, message=FALSE}
library(tidyverse)
library(caret)
library(elasticnet)
```

# Importing Data. Splitting Data

The data is downloaded into the current directory and loaded into R. The current directory can be found using `getwd()`.

```{r import}
if(!file.exists("./barbell.csv")) {
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                      destfile = "./barbell.csv", method = "curl")
}
barbell <- read.csv("./barbell.csv")
barbell$classe <- as.factor(barbell$classe)
```

We now split the data into train, validation and test sets.

```{r split}
set.seed(4567)
inBuild <- createDataPartition(barbell$classe, p = 0.7, list = FALSE)
build <- as_tibble(barbell[inBuild,])
test <- as_tibble(barbell[-inBuild,])

set.seed(3564)
inTrain <- createDataPartition(build$classe, p = 0.7, list = FALSE)
train <- as_tibble(build[inTrain,])
validation <- as_tibble(build[-inTrain,])
```

From now on, the test data is not touched. We will compare models' accuracies on the validation data.

# Exploratory Data Analysis

```{r explore}
dim(train)
summary(train$classe)
```

We can see how `train$classe` is distributed

```{r ABCDE, fig.height=4, fig.width=5, fig.align='center'}
ggplot(train, aes(x = classe)) + theme_minimal() +
        geom_bar(aes(fill = classe), show.legend = FALSE) +
        scale_fill_manual(values = c("orangered3", "palegreen4", "palegreen3", "palegreen2", "palegreen1")) +
        labs(title = "Classe Distribution", x = "Classe", y = "Count")
```

When viewing the data set, it is obvious that some variables are just participant classifiers or summary statistics. Therefore, they will not be used to build a model. These variable numbers are assigned to the object `exl_var`.

```{r exl_var}
exl_var <- c(1:7, 12:36, 50:59, 69:83, 87:101, 103:112, 125:139, 141:150)
```

These are variables like:

```{r what}
head(names(train[,exl_var]))
```

Hence, the model will use:

```{r real_preds}
length(names(train)) - length(exl_var) - 1 # -1 as this is the outcome variable
```

52 predictors.

The remaining variables have the following classes:

```{r var_class}
table(as.factor(sapply(train[,-c(exl_var, 160)], class))) # variable 160 is the outcome
```

Was this the right thing to do?

```{r nsv, cache=TRUE}
nearZeroVar(train) %in% exl_var
exl_var %in% nearZeroVar(train)
mean(exl_var %in% nearZeroVar(train))
```

Possibly the variable reduction was a bit extreme. We shall see how it impacts predictions.

We can have see whether there is any trend of a given predictor variable vs the index. These plots are coloured by `classe`.

```{r plots, fig.height=9, fig.width=7, fig.align='center', cache=TRUE}
par(mfrow = c(8, 7), mar = c(2, 2, 1, 1))
for(i in 1:ncol(train[,-c(exl_var, 160)])) {
        x <- 1:length(train$classe)
        y <- train[,-c(exl_var, 160)][, i, drop = TRUE]
        plot(x, y, col = train$classe)
}
```

Some clustering and grouping implies we can use decision trees, random forest, boosting or linear discriminant analysis methods, amongst others. The evident trend between index and classe is due to participants lifting the barbell a given way in a given order.

# Building the Model

All the 52 predictor variables will be used. First, let's fit a decision tree model:

```{r rpart, cache=TRUE}
set.seed(2837)
rpart_model1 <- train(classe ~ ., data = train[,-exl_var], method = "rpart")
confusionMatrix(predict(rpart_model1, validation[,-exl_var]), validation$classe)$overall[1]
```

This is not a very good model.

Now, let's fit a random forest model with bootstrap, 0.632bootstrap and k-fold cross-validation.

```{r rf, cache=TRUE}
# bootstrap
set.seed(818)
rf_model1 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "boot", number = 25))
confusionMatrix(predict(rf_model1, validation[,-exl_var]), validation$classe)$overall[1]

# bootstrap with more resamples
set.seed(818)
rf_model2 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "boot", number = 50))
confusionMatrix(predict(rf_model2, validation[,-exl_var]), validation$classe)$overall[1]

# 0.632bootstrap
set.seed(818)
rf_model3 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "boot632", number = 25))
confusionMatrix(predict(rf_model3, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 10 folds
set.seed(873)
rf_model4 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 10))
confusionMatrix(predict(rf_model4, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 20 folds
set.seed(873)
rf_model5 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 20))
confusionMatrix(predict(rf_model5, validation[,-exl_var]), validation$classe)$overall[1]
```

It is interesting to note that with higher numbers of resamples, the model starts to overfit the data and performs worse when predicting the validation outcome. The 0.632bootstrap `rf_model3`, despite being slightly worse at predicting on the validation data, is preferred as it does not underestimate error. It and `rf_model5` might be stacked later.

Random forests seem to do a great job. Let's see whether fitting a linear discriminant analysis model will increase the accuracy. All parameters from `rf_model3` and `rf_model5` remain the same, except for the modelling method.

```{r lda, cache=TRUE}
# 0.632bootstrap
set.seed(818)
lda_model1 <- train(classe ~ ., data = train[,-exl_var], method = "lda",
                   trControl = trainControl(method = "boot632", number = 25))
confusionMatrix(predict(lda_model1, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 20 folds
set.seed(873)
lda_model2 <- train(classe ~ ., data = train[,-exl_var], method = "lda",
                   trControl = trainControl(method = "cv", number = 20))
confusionMatrix(predict(lda_model2, validation[,-exl_var]), validation$classe)$overall[1]
```

Linear discriminant analysis does not work well for this data.

From the models we have tried, random forest has worked best. Let's see the impact of changing the k parameter in `rf_model5`:

```{r rf_k, cache=TRUE}
# k-fold with 30 folds
set.seed(873)
rf_model6 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 30))
confusionMatrix(predict(rf_model6, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 40 folds
set.seed(873)
rf_model7 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 40))
confusionMatrix(predict(rf_model7, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 50 folds
set.seed(873)
rf_model8 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 50))
confusionMatrix(predict(rf_model8, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 100 folds
set.seed(873)
rf_model9 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 10, trControl = trainControl(method = "cv", number = 100))
confusionMatrix(predict(rf_model9, validation[,-exl_var]), validation$classe)$overall[1]
```

It seems that changing k does not significantly alter accuracy. One final parameter to explore is the number of trees. This significantly affects computational run time, so ntree > 15 will not be explored.

```{r rf_ntree, cache=TRUE}
# 0.632bootstrap, ntree = 5
set.seed(818)
rf_model10 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 5, trControl = trainControl(method = "boot632", number = 25))
confusionMatrix(predict(rf_model10, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 20 folds, ntree = 5
set.seed(873)
rf_model11 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 5, trControl = trainControl(method = "cv", number = 20))
confusionMatrix(predict(rf_model11, validation[,-exl_var]), validation$classe)$overall[1]

# 0.632bootstrap, ntree = 15
set.seed(818)
rf_model12 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 15, trControl = trainControl(method = "boot632", number = 25))
confusionMatrix(predict(rf_model12, validation[,-exl_var]), validation$classe)$overall[1]

# k-fold with 20 folds, ntree = 15
set.seed(873)
rf_model13 <- train(classe ~ ., data = train[,-exl_var], method = "rf",
                   ntree = 15, trControl = trainControl(method = "cv", number = 20))
confusionMatrix(predict(rf_model13, validation[,-exl_var]), validation$classe)$overall[1]
```

It is no suprise that more trees means greater accuracy. The original `rf_model3` and `rf_model5` will now be stacked. `rf_model12` and `rf_model13` are not used, despite greater accuracy, due to the heightened run time. A generalised additive modelling method will be used for the stacking:

```{r gam, message=FALSE, warning=FALSE, cache=TRUE}
df_pred <- tibble(pred_rf3 = predict(rf_model3, train[,-exl_var]),
                  pred_rf5 = predict(rf_model5, train[,-exl_var]),
                  classe = train$classe)

gam_model <- train(classe ~ ., data = df_pred, method = "gam")
confusionMatrix(predict(gam_model, tibble(pred_rf3 = predict(rf_model3, validation[,-exl_var]),
                                          pred_rf5 = predict(rf_model5, validation[,-exl_var]),
                                          classe = validation$classe)),
                validation$classe)$overall[1]
```

It might be the case that the predictors are now collinear and therefore the generalised additive model is not very accurate. Hence, the most accurate model from all the ones fitted so far, `rf_model13`, will be the final model. Since we are using only one model, we can sacrifice time for accuracy.

# Conclusion

The reasons for picking the final model have been outlined above. Let's see how accurate it is on the test data and therefore estimate the out of sample error.

```{r final}
test_pred <- predict(rf_model13, test[,-exl_var])

final_acc <- confusionMatrix(test_pred, test$classe)$overall
```

This is a fantastic accuracy!

The out of sample error estimate is:

```{r oos}
unname(1 - final_acc[1])
```

# Course Project Prediction

The data for the course prediction quiz is downloaded below and the final model is used to predict the class

```{r prediction}
if(!file.exists("./prediction.csv")) {
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                      destfile = "./prediction.csv", method = "curl")
}

prediction <- read.csv("./prediction.csv")
predict(rf_model13, prediction[,-exl_var])
```

Having taken the quiz I got one wrong, so a `r 19/20` accuracy, which is very close to the final accuracy of `r round(final_acc[1], 3)`.